# -*- coding: utf-8 -*-
"""
EF-AI ‚Äî Multi-agent UI (Gradio) sin Guardrails
Pesta√±as:
  - Chat RAG (consulta √≠ndice)
  - Indexador (subir/a√±adir docs)
  - Generador de R√∫bricas (YAML -> DOCX)
  - Corrector (aplica r√∫brica sobre texto del alumno)
  - Ajustes (modelos Ollama, salud del servidor)
"""

import os, shutil, json, textwrap
from pathlib import Path
from datetime import datetime
from typing import List, Optional

import requests
import gradio as gr
from dotenv import load_dotenv

# Carga variables .env
load_dotenv()

# --- RAG backend ---
from ef_rag import INDEX_DIR, add_to_index, load_index, make_qa, DEFAULT_MODEL

# PII scrub opcional
try:
    from privacy import scrub as scrub_pii
except Exception:
    def scrub_pii(x: str) -> str:
        return x

# --------- Constantes / rutas ----------
UPLOAD_STAGING = Path("uploads")
UPLOAD_STAGING.mkdir(exist_ok=True)
OUTPUTS_DIR = Path("outputs")
OUTPUTS_DIR.mkdir(exist_ok=True)

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:11434")
DEFAULT_LLM = os.getenv("EF_AI_MODEL", DEFAULT_MODEL)

# --------- Utilidades Ollama ----------
def ollama_models() -> List[str]:
    """Devuelve la lista de modelos instalados en Ollama (o fallback)."""
    try:
        r = requests.get(f"{BASE_URL}/api/tags", timeout=2)
        r.raise_for_status()
        data = r.json()
        models = [m["name"] for m in data.get("models", [])]
        return sorted(set(models)) or [DEFAULT_LLM]
    except Exception:
        return [DEFAULT_LLM]

def ollama_health() -> str:
    try:
        r = requests.get(f"{BASE_URL}/api/tags", timeout=2)
        r.raise_for_status()
        return "üü¢ Ollama: OK"
    except requests.exceptions.ConnectionError:
        return "üî¥ Ollama: servidor no accesible (arranca con: `ollama serve`)"
    except Exception as e:
        return f"üü† Ollama: {type(e).__name__}"

# --------- Indexado ----------
def index_uploads(files, unidad):
    if not files:
        return gr.Info("Sube alg√∫n archivo primero."), None
    saved = []
    for f in files:
        dst = UPLOAD_STAGING / f.name
        shutil.copyfile(f.name, dst)
        saved.append(str(dst))
    meta = {"unidad": unidad or "General"}
    add_to_index(saved, default_meta=meta)
    return gr.Success(f"Indexados {len(saved)} archivo(s) en '{INDEX_DIR}'."), None

def clear_session_index():
    for p in UPLOAD_STAGING.glob("*"):
        try:
            p.unlink()
        except Exception:
            pass
    return gr.Warning("Subidas temporales limpiadas (el √≠ndice FAISS global no se borra).")

# --------- Chat RAG ----------
def ask_rag(history, question, unidad, k, model_name):
    q = (question or "").strip()
    if not q:
        return history, gr.Info("Escribe una pregunta."), f"Usando k={k}. Contextos: 0.", ""
    q = scrub_pii(q)

    # Carga √≠ndice
    try:
        db = load_index()
    except Exception as e:
        return history, gr.Error("No existe el √≠ndice. A√±ade documentos primero."), f"Error: {e}", ""

    # QA
    model = model_name or DEFAULT_LLM
    qa = make_qa(db, model_name=model, temperature=0.2)
    try:
        ans = qa.run(q, k=int(k or 3), unidad=(unidad or None), show=False)
    except Exception as e:
        return history, gr.Error(f"Error LLM: {e}"), f"Modelo: {model}", ""

    # Chatbot (messages)
    history = history + [
        {"role": "user", "content": q},
        {"role": "assistant", "content": ans},
    ]
    context_text = f"Usando k={k}. Modelo: {model}"
    return history, None, context_text, ""

# --------- Generador de r√∫bricas ----------
def generate_rubric_from_yaml(yaml_text: str, course: str, unit: str) -> str:
    """Crea un DOCX simple a partir de YAML de r√∫brica."""
    import yaml
    from docx import Document

    if not yaml_text.strip():
        raise ValueError("Pegue un YAML de r√∫brica.")
    y = yaml.safe_load(yaml_text)

    doc = Document()
    doc.add_heading(f"Rubric ‚Äì {course} ‚Äì {unit}", level=1)

    dims = y.get("dimensions", [])
    if not isinstance(dims, list):
        dims = []

    for block in dims:
        name = block.get("name", "Dimension")
        doc.add_heading(name, level=2)
        for lvl in block.get("levels", []):
            label = lvl.get("label", "Level")
            desc = lvl.get("desc", "")
            doc.add_paragraph(f"{label}: {desc}")

    OUTPUTS_DIR.mkdir(exist_ok=True)
    out = OUTPUTS_DIR / f"rubric_{course}_{unit}_{datetime.now():%Y%m%d_%H%M}.docx"
    doc.save(out)
    return str(out)

# --------- Corrector (r√∫brica + tarea) ----------
def correct_one_llm(model_name: str, rubric_text: str, student_text: str) -> str:
    """Invoca el LLM con un prompt compacto para devolver JSON con score & reasons."""
    from langchain_ollama import ChatOllama
    if not rubric_text.strip() or not student_text.strip():
        return json.dumps({"error": "Faltan r√∫brica o texto del estudiante."}, ensure_ascii=False)

    sys_prompt = (
        "Eres un evaluador de Educaci√≥n F√≠sica en Secundaria. Eval√∫a la tarea usando esta r√∫brica (escala 1‚Äì4).\n"
        "Devuelve SOLO JSON con las claves: score (1-4) y reasons (‚â§40 palabras, espa√±ol, EF)."
    )
    user = (
        f"R√öBRICA (YAML):\n{rubric_text}\n\n"
        f"TAREA DEL ESTUDIANTE:\n{student_text}\n\n"
        "Estructura JSON exacta:\n"
        "{\"score\": <1-4>, \"reasons\": \"<‚â§40 palabras>\"}"
    )
    llm = ChatOllama(model=model_name or DEFAULT_LLM, temperature=0.2)
    msg = f"<sistema>\n{sys_prompt}\n</sistema>\n<usuario>\n{user}\n</usuario>"
    try:
        out = llm.invoke(msg).content.strip()
    except Exception as e:
        out = json.dumps({"error": f"LLM: {e}"}, ensure_ascii=False)
    return out

# --------- UI  ----------
THEME_CSS = """
:root {
  --glass-bg: rgba(255,255,255,0.7);
  --glass-brd: rgba(255,255,255,0.35);
  --accent: #ff7a59;
}
.gradio-container {background: linear-gradient(135deg, #eef2ff 0%, #e0f7ff 100%) fixed;}
.card {
  background: var(--glass-bg);
  backdrop-filter: blur(10px);
  border: 1px solid var(--glass-brd);
  border-radius: 14px;
  padding: 14px;
}
#titlebar {font-weight:700; font-size: 22px;}
.btn-primary {background: var(--accent) !important; border: none;}
"""

def build_ui():
    with gr.Blocks(theme=gr.themes.Soft(), css=THEME_CSS, title="EF-AI ‚Äî Agents") as demo:
        # Header con salud
        with gr.Row():
            with gr.Column():
                gr.Markdown("<div id='titlebar'>üè´ EF-AI ‚Äî Agents (Ollama local)</div>")
                health = gr.Markdown(ollama_health())
        with gr.Tabs():
            # ---------------- Chat RAG ----------------
            with gr.Tab("üîé Chat RAG"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### Controles", elem_classes=["card"])
                        unidad = gr.Textbox(value="General", label="Unidad/Tag")
                        k = gr.Slider(1, 6, value=3, step=1, label="k")
                        model_choice = gr.Dropdown(
                            choices=ollama_models(),
                            value=DEFAULT_LLM,
                            label="Modelo Ollama"
                        )
                    with gr.Column(scale=2):
                        chat = gr.Chatbot(type="messages", height=420, elem_classes=["card"])
                        with gr.Row():
                            question = gr.Textbox(placeholder="Escribe tu query...", label="Pregunta")
                            ask_btn = gr.Button("Preguntar", variant="primary", elem_classes=["btn-primary"])
                        footer = gr.Markdown("Usando k=3. Modelo: "+DEFAULT_LLM)
                        debug_box = gr.Textbox(label="Logs breves", interactive=False)

                ask_btn.click(
                    ask_rag,
                    [chat, question, unidad, k, model_choice],
                    [chat, health, footer, debug_box]
                )

            # ---------------- Indexador ----------------
            with gr.Tab("üìö Indexador"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### A√±adir documentos al √≠ndice", elem_classes=["card"])
                        unidad_idx = gr.Textbox(value="General", label="Unidad/Tag")
                        files = gr.Files(label="Adjunta PDF/DOCX/TXT/MD", file_count="multiple")
                        add_btn = gr.Button("‚ûï A√±adir al √≠ndice", variant="primary", elem_classes=["btn-primary"])
                        clear_btn = gr.Button("üßπ Limpiar subidas (no borra FAISS)")
                        status_idx = gr.Markdown()
                add_btn.click(index_uploads, [files, unidad_idx], [status_idx, files])
                clear_btn.click(clear_session_index, outputs=[status_idx])

            # ---------------- R√∫bricas ----------------
            with gr.Tab("üß© Generador de r√∫bricas"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### Config", elem_classes=["card"])
                        course = gr.Textbox(value="4¬∫ ESO", label="Curso")
                        unit = gr.Textbox(value="Futsal", label="Unidad")
                    with gr.Column(scale=2):
                        yaml_box = gr.Code(label="Pega aqu√≠ el YAML de la r√∫brica", language="yaml", lines=20)
                        gen_btn = gr.Button("Generar DOCX", variant="primary", elem_classes=["btn-primary"])
                        out_path = gr.Textbox(label="Archivo generado", interactive=False)
                def _gen(ytext, c, u):
                    try:
                        p = generate_rubric_from_yaml(ytext, c, u)
                        return gr.Success("R√∫brica generada."), p
                    except Exception as e:
                        return gr.Error(str(e)), ""
                gen_btn.click(_gen, [yaml_box, course, unit], [health, out_path])

            # ---------------- Corrector ----------------
            with gr.Tab("‚úÖ Corrector (r√∫brica + tarea)"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### Modelo", elem_classes=["card"])
                        model_corr = gr.Dropdown(choices=ollama_models(), value=DEFAULT_LLM, label="Modelo Ollama")
                    with gr.Column(scale=2):
                        rubric_text = gr.Code(label="R√∫brica (YAML)", language="yaml", lines=16)
                        student_text = gr.Textbox(label="Tarea del estudiante", lines=6, placeholder="Pega aqu√≠ la respuesta del alumno...")
                        corr_btn = gr.Button("Corregir", variant="primary", elem_classes=["btn-primary"])
                        corr_out = gr.Code(label="Resultado JSON", language="json", lines=10)
                corr_btn.click(lambda m, r, s: correct_one_llm(m, r, s),
                               [model_corr, rubric_text, student_text],
                               [corr_out])

            # ---------------- Ajustes ----------------
            with gr.Tab("‚öôÔ∏è Ajustes"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### Estado del servidor", elem_classes=["card"])
                        refresh_btn = gr.Button("üîÑ Comprobar Ollama")
                        models_box = gr.JSON(label="Modelos instalados")
                    with gr.Column(scale=1):
                        gr.Markdown("#### Variables activas", elem_classes=["card"])
                        env_box = gr.JSON(value={
                            "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", ""),
                            "EF_AI_MODEL": DEFAULT_LLM,
                            "INDEX_DIR": str(INDEX_DIR),
                        })

                def _refresh():
                    return ollama_health(), ollama_models()
                refresh_btn.click(_refresh, None, [health, models_box])

    return demo


if __name__ == "__main__":
    demo = build_ui()
    demo.launch()